<!DOCTYPE html>
<html lang="zh-CN">
<head>
 <!--pjax：防止跳转页面音乐暂停-->
 <script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css"/ media="defer" onload="this.media='all'">
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/qi.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/qi.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/qi.png">
  <link rel="mask-icon" href="/images/qi.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wuliuqi.xyz","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":true,"color":"#F6E0C9","save":"auto"},"fancybox":"ture","mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="安装scrapy框架 安装scrapy：pip install scrapy windows下，还需要安装pypiwin32，防止报错。pip install scrapy。报Visual C++ 14.0 is required错误的话，安装Visual C++ Build Tools，Visual C++ Build Tools 2015下载地址，下载链接在网页的中间位置  创建项目和爬虫 创">
<meta property="og:type" content="article">
<meta property="og:title" content="爬虫笔记 - scrapy">
<meta property="og:url" content="http://wuliuqi.xyz/2019/02/17/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="July&#39;s Blog">
<meta property="og:description" content="安装scrapy框架 安装scrapy：pip install scrapy windows下，还需要安装pypiwin32，防止报错。pip install scrapy。报Visual C++ 14.0 is required错误的话，安装Visual C++ Build Tools，Visual C++ Build Tools 2015下载地址，下载链接在网页的中间位置  创建项目和爬虫 创">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://developers.whatismybrowser.com/">
<meta property="article:published_time" content="2019-02-17T07:25:48.000Z">
<meta property="article:modified_time" content="2019-12-29T12:11:02.000Z">
<meta property="article:author" content="July">
<meta property="article:tag" content="scrapy">
<meta property="article:tag" content="redis">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://developers.whatismybrowser.com/">

<link rel="canonical" href="http://wuliuqi.xyz/2019/02/17/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>爬虫笔记 - scrapy | July's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
<script defer src="/live2d-widget/autoload.js"></script>
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">July's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">花有重开日，人无再少年</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-photos">

    <a href="/photos/" rel="section"><i class="fas fa-camera-retro fa-fw"></i>相册</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://wuliuqi.xyz/2019/02/17/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/memory.jpg">
      <meta itemprop="name" content="July">
      <meta itemprop="description" content="落花似人有情这个季节">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="July's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          爬虫笔记 - scrapy
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-02-17 15:25:48" itemprop="dateCreated datePublished" datetime="2019-02-17T15:25:48+08:00">2019-02-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-12-29 20:11:02" itemprop="dateModified" datetime="2019-12-29T20:11:02+08:00">2019-12-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>14k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>13 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="安装scrapy框架"><a href="#安装scrapy框架" class="headerlink" title="安装scrapy框架"></a>安装<code>scrapy</code>框架</h1><ol>
<li>安装<code>scrapy</code>：<code>pip install scrapy</code></li>
<li>windows下，还需要安装<code>pypiwin32</code>，防止报错。<code>pip install scrapy</code>。报<code>Visual C++ 14.0 is required</code>错误的话，安装<code>Visual C++ Build Tools</code>，<a target="_blank" rel="noopener" href="https://blogs.msdn.microsoft.com/pythonengineering/2016/04/11/unable-to-find-vcvarsall-bat/">Visual C++ Build Tools 2015下载地址</a>，下载链接在网页的中间位置</li>
</ol>
<h1 id="创建项目和爬虫"><a href="#创建项目和爬虫" class="headerlink" title="创建项目和爬虫"></a>创建项目和爬虫</h1><ol>
<li>创建项目：<code>scapy startproject [项目名称]</code></li>
<li>创建爬虫：命令行下，进入项目所在的路径，执行命令<code>scapy genspider [爬虫名字] [爬取的域名]</code>。注意：爬虫的名字不能和项目名字一样。</li>
</ol>
<span id="more"></span>

<h1 id="项目目录结构"><a href="#项目目录结构" class="headerlink" title="项目目录结构"></a>项目目录结构</h1><ol>
<li><code>items.py</code>：用来存放爬虫爬取下来数据的模型。</li>
<li><code>middlewares.py</code>：用来存放各种中间件的文件。</li>
<li><code>pipelines.py</code>：用来将items的模型存储到本地磁盘中。</li>
<li><code>settings.py</code>：本爬虫的一些配置信息（比如请求头、多久发送一次请求、ip代理池等）。</li>
<li><code>scrapy.cfg</code>：项目的配置文件。</li>
<li><code>spiders包</code>：以后所有的爬虫，都是存放到这个里面。</li>
</ol>
<h1 id="修改settings-py代码"><a href="#修改settings-py代码" class="headerlink" title="修改settings.py代码"></a>修改<code>settings.py</code>代码</h1><p>在做一个爬虫之前，一定要记得修改<code>setttings.py</code>中的设置。两个地方是强烈建议设置的。</p>
<ol>
<li><code>ROBOTSTXT_OBEY</code>设置为<code>False</code>。默认是<code>True</code>。即遵守机器协议，那么在爬虫的时候，<code>scrapy</code>首先去找<code>robots.txt</code>文件，如果没有找到。则直接停止爬取。</li>
<li><code>DEFAULT_REQUEST_HEADERS</code>添加<code>User-Agent</code>。这个也是告诉服务器，我这个请求是一个正常的请求，不是一个爬虫。</li>
<li>如果要激活<code>pipeline</code>，应该在<code>settings.py</code>中，设置<code>ITEM_PIPELINES</code>。</li>
<li>设置延时时间，<code>DOWNLOAD_DELAY = 1</code></li>
</ol>
<h1 id="运行scrapy项目"><a href="#运行scrapy项目" class="headerlink" title="运行scrapy项目"></a>运行scrapy项目</h1><p>运行<code>scrapy</code>项目。需要在终端，进入项目所在的路径，然后<code>scrapy crawl [爬虫名字]</code>即可运行指定的爬虫。如果不想每次都在命令行中运行，那么可以把这个命令写在一个文件中。以后就在<code>pycharm</code>中执行运行这个文件就可以了。比如现在新创建一个文件叫做<code>start.py</code>，然后在这个文件中填入以下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> cmdline</span><br><span class="line"></span><br><span class="line">cmdline.execute(<span class="string">&quot;scrapy crawl qsbk&quot;</span>.split())</span><br></pre></td></tr></table></figure>

<h1 id="糗事百科Scrapy爬虫笔记"><a href="#糗事百科Scrapy爬虫笔记" class="headerlink" title="糗事百科Scrapy爬虫笔记"></a>糗事百科Scrapy爬虫笔记</h1><ol>
<li><code>response</code>是一个<code>scrapy.http.response.html.HtmlResponse</code>对象。可以执行<code>xpath</code>和<code>css</code>语法提取数据。</li>
<li>提取出来的数据，是一个<code>Selector</code>或者是一个<code>SelectorList</code>对象。如果想要获取其中的字符串。使用<code>getall</code>或者<code>get</code>方法。</li>
<li><code>getall</code>方法：获取<code>Selector</code>中的所有文本。返回的是一个列表。</li>
<li><code>get</code>方法：获取的是<code>Selector</code>中的第一个文本。返回的是一个<code>str</code>类型。</li>
<li>如果数据解析回来，要传给<code>pipeline</code>处理。那么可以使用<code>yield</code>来返回。或者是收集所有的<code>item</code>。最后统一使用return返回。</li>
<li><code>item</code>：建议在<code>items.py</code>中定义好模型。以后就不要使用字典。</li>
<li><code>pipeline</code>：这个是专门用来保存数据的。其中有三个方法是会经常用的。<ul>
<li><code>open_spider(self,spider)</code>：当爬虫被打开的时候执行</li>
<li><code>process_item(self,item,spider</code>)：当爬虫有item传过来的时候会被调用</li>
<li><code>close_spider(self,spider)</code>：当爬虫关闭的时候会被调用</li>
</ul>
</li>
</ol>
<p>要激活<code>pipeline</code>，应该在<code>settings.py</code>中，设置<code>ITEM_PIPELINES</code>。示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;qsbk.pipelines.QsbkPipeline&#x27;</span>: <span class="number">300</span>, <span class="comment">#数字小，优先级高</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="JsonItemExporter和JsonLinesItemExporter："><a href="#JsonItemExporter和JsonLinesItemExporter：" class="headerlink" title="JsonItemExporter和JsonLinesItemExporter："></a>JsonItemExporter和JsonLinesItemExporter：</h2><p>保存<code>json</code>数据的时候，可以使用这两个类，让操作变得更简单。</p>
<h3 id="JsonItemExport"><a href="#JsonItemExport" class="headerlink" title="JsonItemExport"></a>JsonItemExport</h3><p>每次把数据添加到内存中。最后统一写到磁盘中。</p>
<ul>
<li>优点：存储的数据是一个满足json规则的数据</li>
<li>缺点：如果数据量比较大，比较耗内存</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonItemExporter</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QsbkPipeline</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.fp = <span class="built_in">open</span>(<span class="string">&#x27;duanzi.json&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>)</span><br><span class="line">        self.exporter = JsonItemExporter(self.fp, ensure_ascii=<span class="literal">False</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        self.exporter.start_exporting()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self,spider</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;爬虫开始&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">        self.exporter.export_item(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.exporter.finish_exporting()</span><br><span class="line">        self.fp.close()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;爬虫结束&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="JsonLinesItemExporter"><a href="#JsonLinesItemExporter" class="headerlink" title="JsonLinesItemExporter"></a>JsonLinesItemExporter</h3><p>每次调用<code>export_item</code>的时候把这个<code>item</code>存在到磁盘中</p>
<ul>
<li>优点：每次处理数据的时候，直接存储到磁盘中，不耗内存。数据也比较安全。</li>
<li>缺点：每一个字典是一行，整个文件不是一个满足<code>json</code>格式的文件。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonLinesItemExporter</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QsbkPipeline</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.fp = <span class="built_in">open</span>(<span class="string">&#x27;duanzi.json&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>)</span><br><span class="line">        self.exporter = JsonLinesItemExporter(self.fp, ensure_ascii=<span class="literal">False</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self,spider</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;爬虫开始&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">        self.exporter.export_item(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.fp.close()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;爬虫结束&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h1 id="CrawlSpider"><a href="#CrawlSpider" class="headerlink" title="CrawlSpider"></a>CrawlSpider</h1><p>可以实现，只要满足某个条件的<code>url</code>，都进行爬取。<code>CrawlSpider</code>继承自<code>Spider</code>，只不过是在之前的基础之上增加了新的功能，可以定义爬取的<code>url</code>的规则，以后<code>scrapy</code>碰到满足条件的<code>url</code>都进行爬取，而不用手动的<code>yield Request</code>。</p>
<h2 id="创建CrawlSpider爬虫"><a href="#创建CrawlSpider爬虫" class="headerlink" title="创建CrawlSpider爬虫"></a>创建<code>CrawlSpider</code>爬虫</h2><p>之前创建爬虫的方式是通过<code>scrapy genspider [爬虫名字] [域名]</code>的方式创建的。如果想要创建<code>CrawlSpider</code>爬虫，那么应该通过以下命令创建：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider -t crawl [爬虫名字] [域名]</span><br></pre></td></tr></table></figure>

<h2 id="LinkExtractors链接提取器"><a href="#LinkExtractors链接提取器" class="headerlink" title="LinkExtractors链接提取器"></a>LinkExtractors链接提取器</h2><p>使用<code>LinkExtractors</code>可以不用程序员自己提取想要的<code>url</code>，然后发送请求。这些工作都可以交给<code>LinkExtractors</code>，他会在所有爬的页面中找到满足规则的<code>url</code>，实现自动的爬取。以下对<code>LinkExtractors</code>类做一个简单的介绍：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">scrapy</span>.<span class="title">linkextractors</span>.<span class="title">LinkExtractor</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    allow = (<span class="params"></span>),</span></span></span><br><span class="line"><span class="class"><span class="params">    deny = (<span class="params"></span>),</span></span></span><br><span class="line"><span class="class"><span class="params">    allow_domains = (<span class="params"></span>),</span></span></span><br><span class="line"><span class="class"><span class="params">    deny_domains = (<span class="params"></span>),</span></span></span><br><span class="line"><span class="class"><span class="params">    deny_extensions = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    restrict_xpaths = (<span class="params"></span>),</span></span></span><br><span class="line"><span class="class"><span class="params">    tags = (<span class="params"><span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;area&#x27;</span></span>),</span></span></span><br><span class="line"><span class="class"><span class="params">    attrs = (<span class="params"><span class="string">&#x27;href&#x27;</span></span>),</span></span></span><br><span class="line"><span class="class"><span class="params">    canonicalize = <span class="literal">True</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    unique = <span class="literal">True</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    process_value = <span class="literal">None</span></span></span></span><br><span class="line"><span class="class"><span class="params"></span>)</span></span><br></pre></td></tr></table></figure>

<p>主要参数讲解：</p>
<ul>
<li><code>allow</code>：允许的url。所有满足这个正则表达式的url都会被提取。</li>
<li><code>deny</code>：禁止的url。所有满足这个正则表达式的url都不会被提取。</li>
<li><code>allow_domains</code>：允许的域名。只有在这个里面指定的域名的url才会被提取。</li>
<li><code>deny_domains</code>：禁止的域名。所有在这个里面指定的域名的url都不会被提取。</li>
<li><code>restrict_xpaths</code>：严格的xpath。和allow共同过滤链接。</li>
</ul>
<h2 id="Rule规则类"><a href="#Rule规则类" class="headerlink" title="Rule规则类"></a>Rule规则类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">scrapy</span>.<span class="title">spiders</span>.<span class="title">Rule</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    link_extractor, </span></span></span><br><span class="line"><span class="class"><span class="params">    callback = <span class="literal">None</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">    cb_kwargs = <span class="literal">None</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">    follow = <span class="literal">None</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">    process_links = <span class="literal">None</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">    process_request = <span class="literal">None</span></span></span></span><br><span class="line"><span class="class"><span class="params"></span>)</span></span><br></pre></td></tr></table></figure>

<p>主要参数讲解：</p>
<ul>
<li><code>link_extractor</code>：一个<code>LinkExtractor</code>对象，用于定义爬取规则。</li>
<li><code>callback</code>：满足这个规则的url，应该要执行哪个回调函数。因为<code>CrawlSpider</code>使用了<code>parse</code>作为回调函数，因此不要覆盖<code>parse</code>作为回调函数自己的回调函数。</li>
<li><code>follow</code>：指定根据该规则从response中提取的链接是否需要跟进。</li>
<li><code>process_links</code>：从<code>link_extractor</code>中获取到链接后会传递给这个函数，用来过滤不需要爬取的链接。</li>
</ul>
<h2 id="使用说明"><a href="#使用说明" class="headerlink" title="使用说明"></a>使用说明</h2><p><code>LinkExtractor</code>和<code>Rule</code>决定爬虫的具体走向。</p>
<ol>
<li><code>allow</code>设置规则的方法：能够限制在我们想要的url上，不与其它的url相同的正则表达式即可。</li>
<li><code>follow</code>使用场景：如果在爬取页面的时候，需要将满足当前条件的url再进行跟进，那么就设置为<code>True</code>。否则设置为<code>False</code>。</li>
<li><code>callback</code>使用场景：如果这个url对应的页面，只是为了获取更多的url，并不需要里面的数据，可以不指定callback。如果想获取url对应页面中的数据，那么久需要制定一个callback。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> wxapp.items <span class="keyword">import</span> WxappItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WxappSpiderSpider</span>(<span class="params">CrawlSpider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;wxapp_spider&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;wxapp-union.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.wxapp-union.com/portal.php?mod=list&amp;catid=2&amp;page=1&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;.+mod=list&amp;catid=2&amp;page=/d&#x27;</span>), follow=<span class="literal">True</span>),</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;.+article-.+\.html&#x27;</span>), callback=<span class="string">&#x27;parse_detail&#x27;</span>, follow=<span class="literal">False</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        title = response.xpath(<span class="string">&quot;//h1[@class=&#x27;ph&#x27;]/text()&quot;</span>).get()</span><br><span class="line">        author_p = response.xpath(<span class="string">&quot;//p[@class=&#x27;authors&#x27;]&quot;</span>)</span><br><span class="line">        author = author_p.xpath(<span class="string">&quot;./a/text()&quot;</span>).get()</span><br><span class="line">        pub_time = author_p.xpath(<span class="string">&quot;./span/text()&quot;</span>).get()</span><br><span class="line">        article_content = response.xpath(<span class="string">&quot;//td[@id=&#x27;article_content&#x27;]//text()&quot;</span>).getall()</span><br><span class="line">        content = <span class="string">&#x27;&#x27;</span>.join(article_content).strip()</span><br><span class="line">        item = WxappItem(author=author, title=title, pub_time=pub_time,content=content)</span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>

<h1 id="Scrapy-Shell"><a href="#Scrapy-Shell" class="headerlink" title="Scrapy Shell"></a>Scrapy Shell</h1><p>我们想要在爬虫中使用<code>xpath</code>、<code>beautifulsoup</code>、<code>正则表达式</code>、<code>css选择器</code>等来提取想要的数据。但是因为<code>scrapy</code>是一个比较重的框架。每次运行起来都要等待一段时间。因此要去验证我们写的提取规则是否正确，是一个比较麻烦的事情。因此<code>scrapy</code>提供了一个<code>shell</code>，用来方便的测试规则。当然也不仅仅局限于这一个功能。</p>
<h2 id="打开Scrapy-Shell"><a href="#打开Scrapy-Shell" class="headerlink" title="打开Scrapy Shell"></a>打开Scrapy Shell</h2><ol>
<li>打开<code>cmd</code>终端，进入到<code>scrapy</code>框架所在的虚拟环境中，输入命令<code>scrapy shell [链接]</code>。就会进入到<code>scrapy</code>的<code>shell</code>环境中。在这个环境中，你可以跟在爬虫的<code>parse</code>方法中一样使用了</li>
<li>如果想读取某个项目的配置信息，应先进入到这个项目目录，再执行<code>scrapy shell</code>命令</li>
</ol>
<h1 id="Request和Response对象"><a href="#Request和Response对象" class="headerlink" title="Request和Response对象"></a>Request和Response对象</h1><h2 id="Request对象"><a href="#Request对象" class="headerlink" title="Request对象"></a>Request对象</h2><p><code>Request</code>对象在我们写爬虫，爬取一页的数据需要重新发送一个请求的时候调用。这个类需要传递一些参数，其中比较常用的参数有：</p>
<ol>
<li><code>url</code>：这个<code>request</code>对象发送请求的<code>url</code>。</li>
<li><code>callback</code>：在下载器下载完成相应的数据后执行的回调函数</li>
<li><code>method</code>：请求的方法。默认为<code>GET</code>方法，可以设置为其它方法。</li>
<li><code>headers</code>：请求头，对于一些固定的设置，放在<code>settings.py</code>中指定就可以了。对于那些非固定的，可以在发送请求的时候指定。</li>
<li><code>meta</code>：比较常用。用于在不同的请求之间传递数据用的</li>
<li><code>encoding</code>：编码。默认为<code>utf-8</code>，使用默认的即可。</li>
<li><code>dot_filter</code>：表示不由调度器过滤。在执行多次重复的请求时用得比较多。</li>
<li><code>errback</code>：在发生错误时执行的函数。</li>
</ol>
<h2 id="Response"><a href="#Response" class="headerlink" title="Response"></a>Response</h2><p><code>Response</code>对象一般是由<code>Scrapy</code>自动构建的。因此开发者不需要关心如何创建<code>Response</code>对象，而是要了解如何使用。<code>Response</code>对象有很多属性，可以用来提取数据：</p>
<ol>
<li><code>meta</code>：从其它请求传过来的<code>meta</code>属性，可以用来保持多个请求之间的数据连接。</li>
<li><code>encoding</code>：返回当前字符串编码和解码的格式</li>
<li><code>text</code>：将返回来的数据作为<code>Unicode</code>字符串返回</li>
<li><code>body</code>：将返回的数据作为<code>bytes</code>字符串返回</li>
<li><code>xpath</code>：<code>xpath</code>选择器</li>
<li><code>css</code>：<code>css</code>选择器</li>
</ol>
<h1 id="发送POST请求"><a href="#发送POST请求" class="headerlink" title="发送POST请求"></a>发送POST请求</h1><p>有时候想要在请求数据的时候发送<code>post</code>请求，那么这时候需要使用<code>Request</code>的子类<code>FormRequest</code>来实现。如果想要在爬虫一开始的时候就发送<code>POST</code>请求，需要在爬虫类中重写<code>start_request(self)</code>方法，并且不再调用<code>start_urls</code>里的<code>url</code>。</p>
<h1 id="下载文件和图片"><a href="#下载文件和图片" class="headerlink" title="下载文件和图片"></a>下载文件和图片</h1><p><code>Scrapy</code>为下载<code>item</code>中包含的文件（比如图片）提供了一个可重用的<code>item pipelines</code>。这些<code>pipeline</code>有些共同的方法和结构（我们称之为<code>media pipeline</code>）。一般来说会使用<code>Files Pipeline</code>或<code>Images Pipeline</code>。</p>
<h2 id="scrapy内置下载文件方法的优点"><a href="#scrapy内置下载文件方法的优点" class="headerlink" title="scrapy内置下载文件方法的优点"></a>scrapy内置下载文件方法的优点</h2><ol>
<li>避免重新下载最近已经下载过的数据</li>
<li>可以方便的指定文件存储的路径</li>
<li>可以将下载的图片转换成通用的格式。比如<code>png</code>或<code>jpg</code>。</li>
<li>可以方便的生成缩略图</li>
<li>可以方便的检测图片的宽和高，确保他们满足最小限制</li>
<li>异步下载，效率非常高</li>
</ol>
<h2 id="下载文件的Files-Pipline"><a href="#下载文件的Files-Pipline" class="headerlink" title="下载文件的Files Pipline"></a>下载文件的Files Pipline</h2><p>当使用<code>Files Pipline</code>下载文件的时候，按照以下步骤来完成：</p>
<ol>
<li>定义好一个<code>Item</code>，然后在这个<code>Item</code>中定义两个属性，分别为<code>file_urls</code>以及<code>files</code>。<code>file_urls</code>是用来存储需要下载的图片的<code>url</code>链接，需要给一个列表</li>
<li>当文件下载完成后，会把文件下载的相关信息存储到<code>item</code>的<code>files</code>属性中。比如下载路径、下载的url和文件的校验码等</li>
<li>在配置文件<code>settings.py</code>中配置<code>FILES_STORE</code>，这个配置是用来设置文件下载下来的路径</li>
<li>启动<code>pipeline</code>：在<code>ITEM_PIPELINES</code>中设置<code>scrapy.pipeline.files.FilesPipeline: 1</code></li>
</ol>
<h2 id="下载图片的Images-Pipeline"><a href="#下载图片的Images-Pipeline" class="headerlink" title="下载图片的Images Pipeline"></a>下载图片的Images Pipeline</h2><p>当使用<code>Images Pipeline</code>下载文件的时候，按照以下步骤来完成：</p>
<ol>
<li>定义好一个<code>Item</code>，然后再这个<code>item</code>中定义两个属性，分别为<code>image_urls</code>以及<code>images</code>。<code>image_urls</code>是用来存储需要下载的图片的<code>url</code>链接，需要给一个列表</li>
<li>当文件下载完成后，会把文件下载的相关细信息存储到<code>item</code>的<code>images</code>属性中。比如下载路径、下载的<code>url</code>和图片的校验码等。</li>
<li>在配置文件<code>settings.py</code>中配置<code>IMAGES_STORE</code>，这个配置是用来设置图片下载下来的路径。</li>
<li>启动<code>pipeline</code>：在<code>ITEM_PIPELINES</code>中设置<code>scrapy.pipelines.images.ImagesPipeline: 1</code></li>
</ol>
<h1 id="Downloader-Middlewares（下载器中间件）"><a href="#Downloader-Middlewares（下载器中间件）" class="headerlink" title="Downloader Middlewares（下载器中间件）"></a>Downloader Middlewares（下载器中间件）</h1><p>下载器中间件是引擎和下载器之间通信的中间件。在这个中间件中可以设置代理、更换请求头等来达到反反爬虫的目的。要写下载器中间件，可以在下载器中实现两个方法。一个是<code>process_request(self, request, spider)</code>，这个方法是在请求发送之前会执行，还有一个是<code>process_response(self, request, response, spider)</code>，这个方法是数据下载到引擎之前执行。</p>
<h2 id="process-request-self-request-spider-："><a href="#process-request-self-request-spider-：" class="headerlink" title="process_request(self, request, spider)："></a>process_request(self, request, spider)：</h2><p>这个方法是下载器在发送请求之前会执行的。一般可以在这个里面设置随机代理ip等。</p>
<ol>
<li>参数：</li>
</ol>
<ul>
<li><code>request</code>：发送请求的request对象</li>
<li><code>spider</code>：发送请求的spider对象</li>
</ul>
<ol start="2">
<li>返回值：</li>
</ol>
<ul>
<li>返回<code>None</code>：如果返回<code>None</code>，<code>Scrapy</code>将继续处理该<code>request</code>，执行其它中间件中的相应方法，知道合适的下载器处理函数被调用，</li>
<li>返回<code>Response</code>对象：<code>Scrapy</code>将不会调用任何其它的<code>process_request</code>方法，将直接返回这个<code>response</code>对象。已经激活的中间件的<code>process_response()</code>方法则会在每个<code>response</code>返回时调用。(有疑问)</li>
<li>返回<code>Request</code>对象：不在使用之前的<code>request</code>对象去下载数据，而是根据现在返回的<code>request</code>对象返回数据。</li>
<li>如果这个方法中抛出了异常，则会调用<code>process_exception</code>方法</li>
</ul>
<h2 id="process-response-self-request-response-spider"><a href="#process-response-self-request-response-spider" class="headerlink" title="process_response(self, request, response, spider)"></a>process_response(self, request, response, spider)</h2><p>这个是下载器下载的数据到引擎中间会执行的方法</p>
<ol>
<li>参数：</li>
</ol>
<ul>
<li>request：request对象</li>
<li>response：被处理的response对象</li>
<li>spider：spider对象</li>
</ul>
<ol start="2">
<li>返回值：</li>
</ol>
<ul>
<li>返回<code>Response</code>对象：会将这个新的<code>response</code>对象传给其它中间件，最终传给爬虫</li>
<li>返回<code>Request</code>对象：下载器链被切断，返回的<code>request</code>会重新被下载器调度下载</li>
<li>如果抛出一个异常，那么调用<code>request</code>的<code>errback</code>方法，如果没有指定这个方法，那么会抛出一个异常。</li>
</ul>
<h2 id="随机请求头中间件"><a href="#随机请求头中间件" class="headerlink" title="随机请求头中间件"></a>随机请求头中间件</h2><p>爬虫在频繁访问一个页面的时候，这个请求头如果一直保持一直。那么很容易被服务器发现，从而禁止这个请求头的访问。因此我们要在访问这个页面之前随机的更改请求头，这样才可以避免爬虫被抓。随机更改请求头，可以在下载器中间件中实现。在请求发送给服务器之前，随机的选择一个请求头。示例代码如下：</p>
<p><img data-src="https://developers.whatismybrowser.com/" alt="User_Agent网站"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserAgentDownloadMiddleware</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    USER_AGENT = [</span><br><span class="line">        <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        user_agent = random.choice(self.USER_AGENT)</span><br><span class="line">        request.headers[<span class="string">&#x27;user-agent&#x27;</span>] = user_agent</span><br></pre></td></tr></table></figure>

<h2 id="ip代理池中间件"><a href="#ip代理池中间件" class="headerlink" title="ip代理池中间件"></a>ip代理池中间件</h2><h3 id="购买代理"><a href="#购买代理" class="headerlink" title="购买代理"></a>购买代理</h3><p>芝麻地理，太阳代理等</p>
<h3 id="使用ip代理池"><a href="#使用ip代理池" class="headerlink" title="使用ip代理池"></a>使用ip代理池</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 中间件</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IPProxyDownloadMiddleware</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    PROXIES = [</span><br><span class="line">        <span class="string">&#x27;111.177.188.158:9999&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;117.191.11.102:8080&#x27;</span></span><br><span class="line">    ]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request,spider</span>):</span></span><br><span class="line">        proxy = random.choice(self.PROXIES)</span><br><span class="line">        request.meta[<span class="string">&#x27;proxy&#x27;</span>] = <span class="string">&#x27;http://&#x27;</span> + proxy</span><br></pre></td></tr></table></figure>

<p>使用独享代理的话：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 中间件</span></span><br><span class="line"><span class="comment"># 快代理，独享代理</span></span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IPProxyDownloadMiddleware</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request,spider</span>):</span></span><br><span class="line">        proxy = <span class="string">&#x27;121.199.6.124:16816&#x27;</span></span><br><span class="line">        user_password = <span class="string">&#x27;970138076: reessc13&#x27;</span></span><br><span class="line">        request.meta[<span class="string">&#x27;proxy&#x27;</span>] = <span class="string">&#x27;http://&#x27;</span> + proxy  <span class="comment">#教程里没加&#x27;http://&#x27;</span></span><br><span class="line">        b64_user_password = base64.b64encode(user_password.encode(utf-<span class="number">8</span>))</span><br><span class="line">        request.headers[<span class="string">&#x27;Proxy-Authorization&#x27;</span>] = <span class="string">&#x27;Basic&#x27;</span> + b64_user_password.decode(<span class="string">&#x27;utf-8&#x27;</span>) <span class="comment">#不明白为什么要先编码再解码</span></span><br></pre></td></tr></table></figure>

<h1 id="redis"><a href="#redis" class="headerlink" title="redis"></a>redis</h1><p><code>redis</code>是一种支持分布式的<code>nosql</code>数据库,他的数据是保存在内存中，同时<code>redis</code>可以定时把内存数据同步到磁盘，即可以将数据持久化，并且他比<code>memcached</code>支持更多的数据结构(<code>string</code>,<code>list列表[队列和栈]</code>,<code>set[集合]</code>,<code>sorted set[有序集合]</code>,<code>hash(hash表)</code>)。相关参考文档：<a target="_blank" rel="noopener" href="http://redisdoc.com/index.html">Redis 命令参考</a></p>
<h2 id="redis使用场景："><a href="#redis使用场景：" class="headerlink" title="redis使用场景："></a>redis使用场景：</h2><ol>
<li>登录会话存储：存储在<code>redis</code>中，与<code>memcached</code>相比，数据不会丢失。</li>
<li>排行版/计数器：比如一些秀场类的项目，经常会有一些前多少名的主播排名。还有一些文章阅读量的技术，或者新浪微博的点赞数等。</li>
<li>作为消息队列：比如<code>celery</code>就是使用<code>redis</code>作为中间人。</li>
<li>当前在线人数：还是之前的秀场例子，会显示当前系统有多少在线人数。</li>
<li>一些常用的数据缓存：比如我们的BBS论坛，板块不会经常变化的，但是每次访问首页都要从<code>mysql</code>中获取，可以在<code>redis</code>中缓存起来，不用每次请求数据库。</li>
<li>把前200篇文章缓存或者评论缓存：一般用户浏览网站，只会浏览前面一部分文章或者评论，那么可以把前面200篇文章和对应的评论缓存起来。用户访问超过的，就访问数据库，并且以后文章超过200篇，则把之前的文章删除。</li>
<li>好友关系：微博的好友关系使用redis实现。</li>
<li>发布和订阅功能：可以用来做聊天软件。</li>
</ol>
<h2 id="redis和memcached的比较"><a href="#redis和memcached的比较" class="headerlink" title="redis和memcached的比较"></a>redis和memcached的比较</h2><table>
<thead>
<tr>
<th></th>
<th>memecached</th>
<th>redis</th>
</tr>
</thead>
<tbody><tr>
<td>类型</td>
<td>纯内存数据库</td>
<td>内存磁盘同步数据库</td>
</tr>
<tr>
<td>数据类型</td>
<td>在定义value时就要固定数据类型</td>
<td>不需要</td>
</tr>
<tr>
<td>虚拟内存</td>
<td>不支持</td>
<td>支持</td>
</tr>
<tr>
<td>过期策略</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>存储数据安全</td>
<td>不支持</td>
<td>可以将数据同步到dump.db中</td>
</tr>
<tr>
<td>灾难恢复</td>
<td>不支持</td>
<td>可以将磁盘中的数据恢复到内存中</td>
</tr>
<tr>
<td>分布式</td>
<td>支持</td>
<td>主从同步</td>
</tr>
<tr>
<td>订阅与发布</td>
<td>不支持</td>
<td>支持</td>
</tr>
</tbody></table>
<h2 id="其它机器访问本机redis服务器"><a href="#其它机器访问本机redis服务器" class="headerlink" title="其它机器访问本机redis服务器"></a>其它机器访问本机redis服务器</h2><p>想要让其他机器访问本机的<code>redis</code>服务器。那么要修改<code>redis.conf</code>的配置文件，将<code>bind</code>改成<code>bind [自己的ip地址或者0.0.0.0]</code>，其他机器才能访问。<br><strong>注意</strong>：<code>bind</code>绑定的是本机网卡的ip地址，而不是想让其他机器连接的ip地址。如果有多块网卡，那么可以绑定多个网卡的ip地址。如果绑定到额是<code>0.0.0.0</code>，那么意味着其他机器可以通过本机所有的ip地址进行访问。</p>
<h2 id="对redis的操作"><a href="#对redis的操作" class="headerlink" title="对redis的操作"></a>对redis的操作</h2><p>对<code>redis</code>的操作可以用两种方式，第一种方式采用<code>redis-cli</code>，第二种方式采用编程语言，比如<code>Python</code>、<code>PHP</code>和<code>JAVA</code>等。下面使用<code>redis-cli</code>对<code>redis</code>进行操作。</p>
<h3 id="字符串操作："><a href="#字符串操作：" class="headerlink" title="字符串操作："></a>字符串操作：</h3><p>启动redis：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service redis-server start</span><br></pre></td></tr></table></figure>

<p>连接上redis-server：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-cli -h [ip] -p [端口]</span><br></pre></td></tr></table></figure>

<p>添加：<code>set key value</code>。将字符串值<code>value</code>关联到<code>key</code>。如果<code>key</code>已经持有其他值，<code>set</code>命令就覆写旧值，无视其类型。并且默认的过期时间是永久，即永远不会过期。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set username xiaotuo</span><br></pre></td></tr></table></figure>

<p>删除：<code>del key</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">del username</span><br></pre></td></tr></table></figure>

<p>设置过期时间：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">expire key timeout(单位为秒)</span><br></pre></td></tr></table></figure>

<p>也可以在设置值的时候，一同指定过期时间：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set key value EX timeout</span><br><span class="line"><span class="meta">#</span><span class="bash"> 或下面的写法</span></span><br><span class="line">setex key timeout value</span><br></pre></td></tr></table></figure>

<p>查看过期时间：<code>ttl key</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ttl username</span><br></pre></td></tr></table></figure>

<p>查看当前<code>redis</code>中的所有<code>key</code>：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keys *</span><br></pre></td></tr></table></figure>

<h3 id="列表操作"><a href="#列表操作" class="headerlink" title="列表操作"></a>列表操作</h3><p>在列表左边添加元素：将值<code>value</code>插入到列表<code>key</code>的表头。如果<code>key</code>不存在，一个空列表会被创建并执行<code>lpush</code>操作。当<code>key</code>存在但不是列表类型时，将返回一个错误。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lpush key <span class="keyword">value</span></span><br></pre></td></tr></table></figure>

<p>在列表右边添加元素：将值<code>value</code>插入到列表<code>key</code>的表尾。如果<code>key</code>不存在，一个空列表会被创建并执行<code>rpush</code>操作。当<code>key</code>存在但不是列表类型时，返回一个错误。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpush key <span class="keyword">value</span></span><br></pre></td></tr></table></figure>

<p>查看列表中的元素：返回列表<code>key</code>中指定区间内的元素，区间以偏移量<code>start</code>和<code>stop</code>指定,如果要左边的第一个到最后的一个<code>lrange key 0 -1</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lrange key start stop</span><br></pre></td></tr></table></figure>

<p>移除列表中的元素：</p>
<p>移除并返回列表key的头元素：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lpop key</span><br></pre></td></tr></table></figure>

<p>移除并返回列表的尾元素：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpop key</span><br></pre></td></tr></table></figure>

<p>移除并返回列表<code>key</code>的中间元素：将删除key这个列表中，值为value的元素，count为指定个数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lrem key count value</span><br></pre></td></tr></table></figure>

<p>指定返回第几个元素：将返回<code>key</code>这个列表中，索引为<code>index</code>的这个元素。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lindex key index</span><br></pre></td></tr></table></figure>

<p>获取列表中的元素个数：<code>llen key</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">llen languages</span><br></pre></td></tr></table></figure>

<p>删除指定的元素：<code>lrem key count value</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lrem languages 0 php</span><br></pre></td></tr></table></figure>

<p>根据参数<code>count</code>的值，移除列表中与参数<code>value</code>相等的元素。<code>count</code>的值可以是以下几种：</p>
<ul>
<li>count &gt; 0：从表头开始向表尾搜索，移除与<code>value</code>相等的元素，数量为<code>count</code>。</li>
<li>count &lt; 0：从表尾开始向表头搜索，移除与<code>value</code>相等的元素，数量为<code>count</code>的绝对值。</li>
<li>count = 0：移除表中所有与<code>value</code>相等的值。</li>
</ul>
<h3 id="set集合的操作"><a href="#set集合的操作" class="headerlink" title="set集合的操作"></a>set集合的操作</h3><p>添加元素：<code>sadd set value1 value2....</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sadd team xiaotuo datuo</span><br></pre></td></tr></table></figure>

<p>查看元素：<code>smembers set</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">smembers team</span><br></pre></td></tr></table></figure>

<p>移除元素：<code>srem set member...</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">srem team xiaotuo datuo</span><br></pre></td></tr></table></figure>

<p>查看集合中的元素个数：<code>scard set</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scard team1</span><br></pre></td></tr></table></figure>

<p>获取多个集合的交集：<code>sinter set1 set2</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sinter team1 team2</span><br></pre></td></tr></table></figure>

<p>获取多个集合的并集：<code>sunion set1 set2</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sunion team1 team2</span><br></pre></td></tr></table></figure>

<p>获取多个集合的差集：<code>sdiff set1 set2</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sdiff team1 team2</span><br></pre></td></tr></table></figure>

<h3 id="hash哈希操作"><a href="#hash哈希操作" class="headerlink" title="hash哈希操作"></a>hash哈希操作</h3><p>添加一个新值：<code>hset key field value</code>。将哈希表<code>key</code>中的域<code>field</code>的值设为<code>value</code>。如果<code>key</code>不存在，一个新的哈希表被创建并进行<code>hset</code>操作。如果域<code>field</code>已经存在于哈希表中，旧值将被覆盖。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hset website baidu baidu.com</span><br></pre></td></tr></table></figure>

<p>获取哈希中的<code>field</code>对应的值：<code>hget key field</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hget website baidu</span><br></pre></td></tr></table></figure>

<p>删除<code>field</code>中的某个<code>field</code>：<code>hdel key field</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdel website baidu</span><br></pre></td></tr></table></figure>

<p>获取某个哈希中所有的<code>field</code>和<code>value</code>：<code>hgetall key</code></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hgetall website</span><br></pre></td></tr></table></figure>

<p>获取某个哈希中所有的<code>field</code>：<code>hkeys key</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hkeys website</span><br></pre></td></tr></table></figure>

<p>获取某个哈希中所有的值：<code>hvals key</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hvals website</span><br></pre></td></tr></table></figure>

<p>判断哈希中是否存在某个<code>field</code>：<code>hexists key field</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexists website baidu</span><br></pre></td></tr></table></figure>

<p>获取哈希中键值对的数量：<code>hlen field</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hlen website</span><br></pre></td></tr></table></figure>

<p>其它知识点用到再补充</p>
<h1 id="Scrapy-Redis分布式爬虫组件"><a href="#Scrapy-Redis分布式爬虫组件" class="headerlink" title="Scrapy-Redis分布式爬虫组件"></a>Scrapy-Redis分布式爬虫组件</h1><p><code>Scrapy</code>是一个框架，他本身是不支持分布式的。如果我们想要做分布式的爬虫，就需要借助一个组件叫做<code>Scrapy-Redis</code>，这个组件正是利用了<code>Redis</code>可以分布式的功能，集成到<code>Scrapy</code>框架中，使得爬虫可以进行分布式。可以充分的利用资源（多个ip、更多带宽、同步爬取）来提高爬虫的爬行效率。</p>
<h1 id="分布式爬虫的优点"><a href="#分布式爬虫的优点" class="headerlink" title="分布式爬虫的优点"></a>分布式爬虫的优点</h1><ol>
<li>可以充分利用多台机器的带宽。</li>
<li>可以充分利用多台机器的ip地址。</li>
<li>多台机器做，爬取效率更高。</li>
</ol>
<h1 id="分布式爬虫必须要解决的问题"><a href="#分布式爬虫必须要解决的问题" class="headerlink" title="分布式爬虫必须要解决的问题"></a>分布式爬虫必须要解决的问题</h1><ol>
<li>分布式爬虫是好几台机器在同时运行，如何保证不同的机器爬取页面的时候不会出现重复爬取的问题。</li>
<li>同样，分布式爬虫在不同的机器上运行，在把数据爬完后如何保证保存在同一个地方。</li>
</ol>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>通过<code>pip install scrapy-redis</code>即可安装。</p>
<h1 id="Scrapy-Redis架构"><a href="#Scrapy-Redis架构" class="headerlink" title="Scrapy-Redis架构"></a>Scrapy-Redis架构</h1><p>这块少图，有空了找个高清的。</p>
<p><code>Item Pipeline</code>在接收到数据后发送给了<code>Redis</code>、<code>Scheduler</code>调度器调度数据也是从<code>Redis</code>中来的、并且其实数据去重也是在<code>Redis</code>中做的。</p>
<h1 id="编写Scrapy-Redis分布式爬虫"><a href="#编写Scrapy-Redis分布式爬虫" class="headerlink" title="编写Scrapy-Redis分布式爬虫"></a>编写Scrapy-Redis分布式爬虫</h1><p>要将一个<code>Scrapy</code>项目变成一个<code>Scrapy-redis</code>项目只需修改以下三点就可以了：</p>
<ol>
<li>将爬虫的类从<code>scrapy.Spider</code>变成<code>scrapy_redis.spiders.RedisSpider</code>；或者是从<code>scrapy.CrawlSpider</code>变成<code>scrapy_redis.spiders.RedisCrawlSpider</code>。</li>
<li>将爬虫中的<code>start_urls</code>删掉。增加一个<code>redis_key=&quot;xxx&quot;</code>。这个<code>redis_key</code>是为了以后在<code>redis</code>中控制爬虫启动的。爬虫的第一个<code>url</code>，就是在<code>redis</code>中通过这个发送出去的。</li>
<li>在配置文件中增加如下配置：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scrapy-Redis相关配置</span></span><br><span class="line"><span class="comment"># 确保request存储到redis中</span></span><br><span class="line">SCHEDULER = <span class="string">&quot;scrapy_redis.scheduler.Scheduler&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 确保所有爬虫共享相同的去重指纹</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">&quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置redis为item pipeline，之前的pipeline注释掉</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">&#x27;scrapy_redis.pipelines.RedisPipeline&#x27;</span>: <span class="number">300</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在redis中保持scrapy-redis用到的队列，不会清理redis中的队列，从而可以实现暂停和恢复的功能。</span></span><br><span class="line">SCHEDULER_PERSIST = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置连接redis信息</span></span><br><span class="line">REDIS_HOST = <span class="string">&#x27;127.0.0.1&#x27;</span></span><br><span class="line">REDIS_PORT = <span class="number">6379</span></span><br></pre></td></tr></table></figure>

<h1 id="运行爬虫"><a href="#运行爬虫" class="headerlink" title="运行爬虫"></a>运行爬虫</h1><ol>
<li>在爬虫服务器上。进入爬虫文件所在的路径，然后输入命令：<code>scrapy runspider [爬虫名字]</code>。</li>
<li>在<code>Redis</code>服务器上，推入一个开始的<code>url</code>链接<code>：redis-cli&gt; lpush [redis_key] start_url</code>开始爬取。</li>
</ol>

    </div>

    
    
    
        <div class="reward-container">
  <div>请作者喝冰阔落</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="July 微信支付">
        <p>微信支付</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>July
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://wuliuqi.xyz/2019/02/17/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/" title="爬虫笔记 - scrapy">http://wuliuqi.xyz/2019/02/17/爬虫笔记/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/scrapy/" rel="tag"><i class="fa fa-tag"></i> scrapy</a>
              <a href="/tags/redis/" rel="tag"><i class="fa fa-tag"></i> redis</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/02/09/%E4%B8%83%E5%A4%A9%E5%B8%A6%E4%BD%A0%E7%8E%A9%E8%BD%ACMySQL%E4%B9%8BSQL%E8%AF%AD%E5%8F%A5/" rel="prev" title="七天带你玩转MySQL之SQL语句">
      <i class="fa fa-chevron-left"></i> 七天带你玩转MySQL之SQL语句
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/02/17/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B001/" rel="next" title="爬虫笔记01 - 数据解析">
      爬虫笔记01 - 数据解析 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%89%E8%A3%85scrapy%E6%A1%86%E6%9E%B6"><span class="nav-number">1.</span> <span class="nav-text">安装scrapy框架</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE%E5%92%8C%E7%88%AC%E8%99%AB"><span class="nav-number">2.</span> <span class="nav-text">创建项目和爬虫</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84"><span class="nav-number">3.</span> <span class="nav-text">项目目录结构</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9settings-py%E4%BB%A3%E7%A0%81"><span class="nav-number">4.</span> <span class="nav-text">修改settings.py代码</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BF%90%E8%A1%8Cscrapy%E9%A1%B9%E7%9B%AE"><span class="nav-number">5.</span> <span class="nav-text">运行scrapy项目</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91Scrapy%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0"><span class="nav-number">6.</span> <span class="nav-text">糗事百科Scrapy爬虫笔记</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#JsonItemExporter%E5%92%8CJsonLinesItemExporter%EF%BC%9A"><span class="nav-number">6.1.</span> <span class="nav-text">JsonItemExporter和JsonLinesItemExporter：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#JsonItemExport"><span class="nav-number">6.1.1.</span> <span class="nav-text">JsonItemExport</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JsonLinesItemExporter"><span class="nav-number">6.1.2.</span> <span class="nav-text">JsonLinesItemExporter</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CrawlSpider"><span class="nav-number">7.</span> <span class="nav-text">CrawlSpider</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BACrawlSpider%E7%88%AC%E8%99%AB"><span class="nav-number">7.1.</span> <span class="nav-text">创建CrawlSpider爬虫</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LinkExtractors%E9%93%BE%E6%8E%A5%E6%8F%90%E5%8F%96%E5%99%A8"><span class="nav-number">7.2.</span> <span class="nav-text">LinkExtractors链接提取器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Rule%E8%A7%84%E5%88%99%E7%B1%BB"><span class="nav-number">7.3.</span> <span class="nav-text">Rule规则类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E"><span class="nav-number">7.4.</span> <span class="nav-text">使用说明</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Scrapy-Shell"><span class="nav-number">8.</span> <span class="nav-text">Scrapy Shell</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%93%E5%BC%80Scrapy-Shell"><span class="nav-number">8.1.</span> <span class="nav-text">打开Scrapy Shell</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Request%E5%92%8CResponse%E5%AF%B9%E8%B1%A1"><span class="nav-number">9.</span> <span class="nav-text">Request和Response对象</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Request%E5%AF%B9%E8%B1%A1"><span class="nav-number">9.1.</span> <span class="nav-text">Request对象</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Response"><span class="nav-number">9.2.</span> <span class="nav-text">Response</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%91%E9%80%81POST%E8%AF%B7%E6%B1%82"><span class="nav-number">10.</span> <span class="nav-text">发送POST请求</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%E5%92%8C%E5%9B%BE%E7%89%87"><span class="nav-number">11.</span> <span class="nav-text">下载文件和图片</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#scrapy%E5%86%85%E7%BD%AE%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%E6%96%B9%E6%B3%95%E7%9A%84%E4%BC%98%E7%82%B9"><span class="nav-number">11.1.</span> <span class="nav-text">scrapy内置下载文件方法的优点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%E7%9A%84Files-Pipline"><span class="nav-number">11.2.</span> <span class="nav-text">下载文件的Files Pipline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E5%9B%BE%E7%89%87%E7%9A%84Images-Pipeline"><span class="nav-number">11.3.</span> <span class="nav-text">下载图片的Images Pipeline</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Downloader-Middlewares%EF%BC%88%E4%B8%8B%E8%BD%BD%E5%99%A8%E4%B8%AD%E9%97%B4%E4%BB%B6%EF%BC%89"><span class="nav-number">12.</span> <span class="nav-text">Downloader Middlewares（下载器中间件）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#process-request-self-request-spider-%EF%BC%9A"><span class="nav-number">12.1.</span> <span class="nav-text">process_request(self, request, spider)：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#process-response-self-request-response-spider"><span class="nav-number">12.2.</span> <span class="nav-text">process_response(self, request, response, spider)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E8%AF%B7%E6%B1%82%E5%A4%B4%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="nav-number">12.3.</span> <span class="nav-text">随机请求头中间件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ip%E4%BB%A3%E7%90%86%E6%B1%A0%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="nav-number">12.4.</span> <span class="nav-text">ip代理池中间件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%AD%E4%B9%B0%E4%BB%A3%E7%90%86"><span class="nav-number">12.4.1.</span> <span class="nav-text">购买代理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8ip%E4%BB%A3%E7%90%86%E6%B1%A0"><span class="nav-number">12.4.2.</span> <span class="nav-text">使用ip代理池</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#redis"><span class="nav-number">13.</span> <span class="nav-text">redis</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#redis%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%EF%BC%9A"><span class="nav-number">13.1.</span> <span class="nav-text">redis使用场景：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#redis%E5%92%8Cmemcached%E7%9A%84%E6%AF%94%E8%BE%83"><span class="nav-number">13.2.</span> <span class="nav-text">redis和memcached的比较</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E5%AE%83%E6%9C%BA%E5%99%A8%E8%AE%BF%E9%97%AE%E6%9C%AC%E6%9C%BAredis%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="nav-number">13.3.</span> <span class="nav-text">其它机器访问本机redis服务器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9redis%E7%9A%84%E6%93%8D%E4%BD%9C"><span class="nav-number">13.4.</span> <span class="nav-text">对redis的操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%93%8D%E4%BD%9C%EF%BC%9A"><span class="nav-number">13.4.1.</span> <span class="nav-text">字符串操作：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%97%E8%A1%A8%E6%93%8D%E4%BD%9C"><span class="nav-number">13.4.2.</span> <span class="nav-text">列表操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#set%E9%9B%86%E5%90%88%E7%9A%84%E6%93%8D%E4%BD%9C"><span class="nav-number">13.4.3.</span> <span class="nav-text">set集合的操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hash%E5%93%88%E5%B8%8C%E6%93%8D%E4%BD%9C"><span class="nav-number">13.4.4.</span> <span class="nav-text">hash哈希操作</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Scrapy-Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E7%BB%84%E4%BB%B6"><span class="nav-number">14.</span> <span class="nav-text">Scrapy-Redis分布式爬虫组件</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E7%9A%84%E4%BC%98%E7%82%B9"><span class="nav-number">15.</span> <span class="nav-text">分布式爬虫的优点</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E5%BF%85%E9%A1%BB%E8%A6%81%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">16.</span> <span class="nav-text">分布式爬虫必须要解决的问题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%89%E8%A3%85"><span class="nav-number">17.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Scrapy-Redis%E6%9E%B6%E6%9E%84"><span class="nav-number">18.</span> <span class="nav-text">Scrapy-Redis架构</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BC%96%E5%86%99Scrapy-Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB"><span class="nav-number">19.</span> <span class="nav-text">编写Scrapy-Redis分布式爬虫</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C%E7%88%AC%E8%99%AB"><span class="nav-number">20.</span> <span class="nav-text">运行爬虫</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="July"
      src="/images/memory.jpg">
  <p class="site-author-name" itemprop="name">July</p>
  <div class="site-description" itemprop="description">落花似人有情这个季节</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">88</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 
src="//music.163.com/outchain/player?type=2&id=559735259&auto=1&height=66"></iframe>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">July</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">351k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">5:19</span>
</div>

<script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="theme-info">
  <div class="powered-by"></div>
  本站总访问量<span id="busuanzi_value_site_pv"></span>次
  <span class="post-meta-divider">|</span>
  本站访客数<span id="busuanzi_value_site_uv"></span>人次
  <span class="post-meta-divider">|</span>
  本文总阅读量<span id="busuanzi_value_page_pv"></span>次
</div>



        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/js/local-search.js"></script>













    <div id="pjax">
  

  



    </div>
</body>
</html>
